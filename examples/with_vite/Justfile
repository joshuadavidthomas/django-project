set dotenv-load := true

export DATABASE_URL := env_var_or_default('DATABASE_URL', 'postgres://postgres:postgres@db:5432/postgres')

# List all available commands
_default:
    @just --list

# ----------------------------------------------------------------------
# DEPENDENCIES
# ----------------------------------------------------------------------

# Bootstrap local development environment
bootstrap:
    @just pup
    @just lock
    @just install

# Install dependencies
install *ARGS:
    python -m uv pip install --upgrade -r requirements.txt

# Generate requirements file
lock *ARGS:
    python -m uv pip compile {{ ARGS }} --generate-hashes requirements.in --output-file requirements.txt

# Install and update dependency tools
pup:
    python -m pip install --upgrade pip uv

# Update local development environment
update:
    @just pup
    @just upgrade
    @just install

# Generate and upgrade dependencies
upgrade:
    @just lock --upgrade

# ----------------------------------------------------------------------
# TESTING/TYPES
# ----------------------------------------------------------------------

# Run the Playwright codegen command within the 'app' container
codegen *ARGS:
    docker compose run --rm --no-deps -e DJANGO_VITE_DEV_SERVER_HOST=node app /bin/bash -c "python manage.py runserver 0.0.0.0:8000 & while ! curl -s http://0.0.0.0:8000 > /dev/null; do sleep 1; done; playwright codegen {{ ARGS }}; kill %1"

coverage:
    rm -rf htmlcov
    @just command "python -m coverage run -m pytest && python -m coverage html --skip-covered --skip-empty"

# Run tests using pytest within the 'app' container, with optional arguments
test *ARGS:
    docker compose run --rm --no-deps -e DJANGO_VITE_DEV_SERVER_HOST=node app pytest {{ ARGS }}

# Run tests with Playwright debug mode enabled, in the 'app' container, with optional arguments
test-debug *ARGS:
    docker compose run --rm --no-deps -e DJANGO_VITE_DEV_SERVER_HOST=node -e PWDEBUG=1 app pytest {{ ARGS }}

# Run mypy on project
types:
    @just command python -m mypy .

# ----------------------------------------------------------------------
# DJANGO
# ----------------------------------------------------------------------

# Run a Django management command
manage *COMMAND:
    @just command python -m manage {{ COMMAND }}

# Alias for makemigrations
alias mm := makemigrations

# Generate Django migrations
makemigrations *APPS:
    @just manage makemigrations {{ APPS }}

# Run Django migrations
migrate *ARGS:
    @just manage migrate {{ ARGS }}

# Open a Django shell using django-extensions shell_plus command
shell-plus:
    @just manage shell_plus

# Quickly create a superuser with the provided credentials
createsuperuser USERNAME="admin" EMAIL="" PASSWORD="admin":
    docker compose run --rm --no-deps app /bin/bash -c 'echo "from django.contrib.auth import get_user_model; User = get_user_model(); User.objects.create_superuser('"'"'{{ USERNAME }}'"'"', '"'"'{{ EMAIL }}'"'"', '"'"'{{ PASSWORD }}'"'"') if not User.objects.filter(username='"'"'{{ USERNAME }}'"'"').exists() else None" | python manage.py shell'

# Reset a user's password
resetuserpassword USERNAME="admin" PASSWORD="admin":
    docker compose run --rm --no-deps app /bin/bash -c 'echo "from django.contrib.auth import get_user_model; User = get_user_model(); user = User.objects.get(username='"'"'{{ USERNAME }}'"'"'); user.set_password('"'"'{{ PASSWORD }}'"'"'); user.save()" | python manage.py shell'

# Graph models using django-extensions graph_models command
graph:
    @just manage graph_models users \
        --exclude-models AbstractUser \
        --group-models \
        --output ./docs/applications/images/users.svg

# ----------------------------------------------------------------------
# DOCS
# ----------------------------------------------------------------------

# Build documentation using Sphinx
docs-build LOCATION="docs/_build/html":
    @just _cog
    sphinx-build docs {{ LOCATION }}

# Install documentation dependencies
docs-install:
    python -m uv pip install --upgrade -r docs/requirements.txt

# Generate documentation requirements
docs-lock *ARGS:
    python -m uv pip compile {{ ARGS }} --generate-hashes docs/requirements.in --output-file docs/requirements.txt

# Serve documentation locally
docs-serve:
    #!/usr/bin/env sh
    @just _cog
    if [ -f "/.dockerenv" ]; then
        sphinx-autobuild docs docs/_build/html --host "0.0.0.0"
    else
        sphinx-autobuild docs docs/_build/html --host "localhost"
    fi

# Generate and upgrade documentation dependencies
docs-upgrade:
    @just docs-lock --upgrade

_cog:
    cog -r docs/just.md

# ----------------------------------------------------------------------
# DOCKER
# ----------------------------------------------------------------------

# Build services using docker compose
build *ARGS:
    docker compose build {{ ARGS }}

# Build production stack using docker compose
build-prod:
    docker compose -f compose.yml -f compose.prod.yml build

# Stop and remove all containers, networks, images, and volumes
clean:
    @just down --volumes --rmi local

# Run a command within the 'app' container
command *ARGS:
    docker compose run --rm --no-deps app /bin/bash -c "{{ ARGS }}"

# Open an interactive shell within the 'app' container opens a console
console:
    docker compose run --rm --no-deps app /bin/bash

# Stop and remove all containers defined in docker compose
down *ARGS:
    docker compose down {{ ARGS }}

# Display the logs for containers, optionally using provided arguments (e.g., --follow)
logs *ARGS:
    docker compose logs {{ ARGS }}

# Display the running containers and their statuses
ps:
    docker compose ps

# Pull the latest versions of all images defined in docker compose
pull:
    docker compose pull

# Restart services, optionally targeting specific ones
restart *ARGS:
    docker compose restart {{ ARGS }}

# Open an interactive shell within a specified container (default: 'app')
shell *ARGS="app":
    docker compose run --rm --no-deps {{ ARGS }} /bin/bash

# Open a psql shell within the 'db' container
shell-db:
    docker compose run --rm --no-deps db psql -d {{ DATABASE_URL }}

# Start services using docker compose, defaulting to detached mode
start *ARGS="--detach":
    @just up {{ ARGS }}

# Start the full production stack using docker compose, defaulting to detached mode
start-prod *ARGS="--detach":
    @just up-prod {{ ARGS }}

# Stop services by calling the 'down' command
stop:
    @just down

# Continuously display the latest logs by using the --follow option, optionally targeting specific containers
tail *ARGS:
    @just logs --follow {{ ARGS }}

# Start services using docker compose, with optional arguments
up *ARGS:
    docker compose up {{ ARGS }}

# Start the full production stack using docker compose
up-prod *ARGS:
    docker compose -f compose.yml -f compose.prod.yml up {{ ARGS }}


# ----------------------------------------------------------------------
# POSTGRES BACKUP AND RESTORE
# ----------------------------------------------------------------------

# Dump our local database to file
pg_dump database_url=DATABASE_URL file='db.dump':
    docker compose run \
        --rm \
        --no-deps \
        db \
        pg_dump \
            --dbname {{ database_url }} \
            --file /app/{{ file }} \
            --format=c \
            --verbose

# Dump our production database to file
pg_dump_prod file='production.dump':
    set PROD_DATABASE_URL := "{{ env_var('PROD_DATABASE_URL') }}"
    @just pg_dump ${PROD_DATABASE_URL} {{ file }}

# Restore database backup to our local database
pg_restore file='db.dump':
    docker compose run \
        --rm \
        --no-deps \
        db \
        pg_restore \
            --clean \
            --dbname {{ DATABASE_URL }} \
            --if-exists \
            --no-owner \
            --verbose \
            /app/{{ file }}

# ----------------------------------------------------------------------
# UTILS
# ----------------------------------------------------------------------

# Sync .env* files
envsync:
    #!/usr/bin/env python
    from pathlib import Path

    envfile = Path('.env')
    envfile_example = Path('.env.example')

    if not envfile.exists():
        envfile.write_text(envfile_example.read_text())

    with envfile.open() as f:
        lines = [line for line in f.readlines() if not line.endswith('# envsync: ignore\n')]
        lines = [line.split('=')[0] + '=\n' if line.endswith('# envsync: no-value\n') else line for line in lines]

        lines.sort()
        envfile_example.write_text(''.join(lines))

# Format justfile
fmt:
    just --fmt --unstable

# Run pre-commit on all files
lint:
    pre-commit run --all-files

# ----------------------------------------------------------------------
# COPIER
# ----------------------------------------------------------------------

# Create a copier answers file
copier-copy TEMPLATE_PATH DESTINATION_PATH=".":
    python -m copier copy --trust {{ TEMPLATE_PATH }} {{ DESTINATION_PATH }}

# Recopy the project from the original template
copier-recopy ANSWERS_FILE *ARGS:
    python -m copier recopy --trust --answers-file {{ ANSWERS_FILE }} {{ ARGS }}

# Loop through all answers files and recopy the project using copier
@copier-recopy-all *ARGS:
    for file in `ls .copier/`; do just copier-recopy .copier/$file "{{ ARGS }}"; done

# Update the project using a copier answers file
copier-update ANSWERS_FILE *ARGS:
    python -m copier update --trust --answers-file {{ ANSWERS_FILE }} {{ ARGS }}

# Loop through all answers files and update the project using copier
@copier-update-all *ARGS:
    for file in `ls .copier/`; do just copier-update .copier/$file "{{ ARGS }}"; done
